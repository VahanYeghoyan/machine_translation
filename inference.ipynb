{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:50:48.382326Z",
     "start_time": "2024-11-15T09:50:47.269090Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformer4 import Transformer\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76fefb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3aa13fd8d8c5badc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:50:50.488401Z",
     "start_time": "2024-11-15T09:50:50.486917Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b480ef42b6c32498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:50:51.242216Z",
     "start_time": "2024-11-15T09:50:51.139305Z"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29fdcb64d70cf007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:50:52.929305Z",
     "start_time": "2024-11-15T09:50:52.793041Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'armenian_tokenizer'\n",
    "tokenizer = tf.saved_model.load(model_name).am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83678420008ff42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "# \n",
    "# def find_top_and_bottom_pikl(directory, top_n=6):\n",
    "#     results = []\n",
    "# \n",
    "#     for filename in os.listdir(directory):\n",
    "#         if filename.endswith('.pkl'):\n",
    "#             filepath = os.path.join(directory, filename)\n",
    "#             with open(filepath, 'rb') as f:\n",
    "#                 data = pickle.load(f)\n",
    "#                 if 'val_masked_accuracy' in data and 'loss' in data:\n",
    "#                     accuracy = data['val_masked_accuracy'][-1]\n",
    "#                     loss = data['loss'][-1]\n",
    "#                     loss_count = len(data['loss'])  # Count the number of items in the loss list\n",
    "#                     results.append((filename, accuracy, loss, loss_count))\n",
    "# \n",
    "#     # Sort the results based on accuracy in descending order for top N\n",
    "#     results.sort(key=lambda x: x[1], reverse=True)\n",
    "#     top_results = results[:top_n]\n",
    "# \n",
    "#     # Sort the results based on accuracy in ascending order for bottom N\n",
    "#     results.sort(key=lambda x: x[1])\n",
    "#     bottom_results = results[:top_n]\n",
    "# \n",
    "#     return top_results, bottom_results\n",
    "# \n",
    "# # Specify your directory here\n",
    "# directory_path = '/home/vahan/Documents/machine_translation/history'\n",
    "# \n",
    "# top_pikl_files, bottom_pikl_files = find_top_and_bottom_pikl(directory_path)\n",
    "# if top_pikl_files:\n",
    "#     print(\"Top pickle files with highest val_masked_accuracy:\")\n",
    "#     for idx, (filename, accuracy, loss, loss_count) in enumerate(top_pikl_files, start=1):\n",
    "#         print(f\"{idx}. {filename} with val_masked_accuracy of {accuracy}, loss of {loss}, and {loss_count} loss items\")\n",
    "# else:\n",
    "#     print(\"No pickle files found or no file contains 'val_masked_accuracy' and 'loss' keys\")\n",
    "# \n",
    "# if bottom_pikl_files:\n",
    "#     print(\"\\nPickle files with lowest val_masked_accuracy:\")\n",
    "#     for idx, (filename, accuracy, loss, loss_count) in enumerate(bottom_pikl_files, start=1):\n",
    "#         print(f\"{idx}. {filename} with val_masked_accuracy of {accuracy}, loss of {loss}, and {loss_count} loss items\")\n",
    "# else:\n",
    "#     print(\"No pickle files found or no file contains 'val_masked_accuracy' and 'loss' keys\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e90aa3ff86543b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_indicis = [3, 21, 15, 6, 7, 18]\n",
    "# worst_indicis = [4, 2, 10, 17, 12, 1]\n",
    "# print(\"best_configs\")\n",
    "# for i in best_indicis:\n",
    "#     with open(f\"/home/vahan/Documents/machine_translation/pkls/{i}.pkl\", \"rb\") as f:\n",
    "#         config = pickle.load(f)\n",
    "#         print(config)\n",
    "# \n",
    "# \n",
    "# print()\n",
    "# print(\"worst_configs\")\n",
    "# for i in worst_indicis:\n",
    "#     with open(f\"/home/vahan/Documents/machine_translation/pkls/{i}.pkl\", \"rb\") as f:\n",
    "#         config = pickle.load(f)\n",
    "#         print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b36d18bd4db7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/vahan/Documents/machine_translation/pkls/3.pkl\", \"rb\") as f:\n",
    "#     config = pickle.load(f)\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29c1ad8793c0d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers = config['num_layers']\n",
    "# d_model = config['d_model']\n",
    "# dff = config['dff']\n",
    "# num_heads = config['num_head']\n",
    "# dropout_rate = config['dropout_rate']\n",
    "# batch_size = config['batch_size']\n",
    "# MAX_TOKENS = config['max_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19a40ca878638cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T11:57:15.262538Z",
     "start_time": "2024-07-31T11:57:15.257290Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_layers = 1\n",
    "# d_model = 110\n",
    "# dff = 1024\n",
    "# num_heads = 5\n",
    "# dropout_rate = 0.15\n",
    "# batch_size = 1024\n",
    "# MAX_TOKENS = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8fa8524982e81832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:50:57.738975Z",
     "start_time": "2024-11-15T09:50:57.737326Z"
    }
   },
   "outputs": [],
   "source": [
    "# bs512_225_d_model256_dff_1024\n",
    "num_layers = 1\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 5\n",
    "dropout_rate = 0.15\n",
    "batch_size = 1024\n",
    "MAX_TOKENS = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5364b5ab0d8b839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:50:58.187830Z",
     "start_time": "2024-11-15T09:50:58.115201Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    # encoder_num_layers=encoder_num_layers,\n",
    "    # decoder_num_layers=decoder_num_layers,\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizer.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizer.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2035d6de7e398e80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:50:58.603178Z",
     "start_time": "2024-11-15T09:50:58.601744Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint_dir = f\"/home/vahan/Documents/machine_translation/models/d_model{config['d_model']}_dff{config['dff']}_numhead{config['num_head']}_numlayer1_bs{config['batch_size']}/ckpt/cp.ckpt\"  \n",
    "# print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9eeb121a9d1acc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:53:06.641029Z",
     "start_time": "2024-11-15T09:53:06.638946Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/home/vahan/Documents/machine_translation/bs512_115_d_model128_dff_512_maxtoken_65/cp.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "324916bf1ccb7c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:53:07.629240Z",
     "start_time": "2024-11-15T09:53:07.587681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x76e8aa1e0050>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_weights(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d966022c269075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:53:08.218539Z",
     "start_time": "2024-11-15T09:53:08.122037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " context_input (InputLayer)  [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " target_input (InputLayer)   [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " encoder_2 (Encoder)         (None, None, 128)            4195200   ['context_input[0][0]']       \n",
      "                                                                                                  \n",
      " decoder_2 (Decoder)         (None, None, 128)            4525184   ['target_input[0][0]',        \n",
      "                                                                     'encoder_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, None, 29166)          3762414   ['decoder_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12482798 (47.62 MB)\n",
      "Trainable params: 12482798 (47.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer_model = transformer.build_model(input_shape=(None,), target_shape=(None,))\n",
    "transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95ac91e330b79cf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:51:29.689143Z",
     "start_time": "2024-11-15T09:51:29.684467Z"
    }
   },
   "outputs": [],
   "source": [
    "class Corrector(tf.Module):\n",
    "    def __init__(self, tokenizer, transformer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "        # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "        assert isinstance(sentence, tf.Tensor)\n",
    "        if len(sentence.shape) == 0:\n",
    "            sentence = sentence[tf.newaxis]\n",
    "\n",
    "        sentence = self.tokenizer.tokenize(sentence).to_tensor()\n",
    "\n",
    "        encoder_input = sentence\n",
    "\n",
    "        # As the output language is English, initialize the output with the\n",
    "        # English `[START]` token.\n",
    "        start_end = self.tokenizer.tokenize([''])[0]\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "        # dynamic-loop can be traced by `tf.function`.\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, start)\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "            # Select the last token from the `seq_len` dimension.\n",
    "            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            # Concatenate the `predicted_id` to the output which is given to the\n",
    "            # decoder as its input.\n",
    "\n",
    "            output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "            if predicted_id == end:\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        # The output shape is `(1, tokens)`.\n",
    "        text = tokenizer.detokenize(output)[0]  # Shape: `()`.\n",
    "        # text = text.replace(\"եւ\", \"և\")\n",
    "         \n",
    "\n",
    "        tokens = tokenizer.lookup(output)[0]\n",
    "\n",
    "\n",
    "\n",
    "        # `tf.function` prevents us from using the attention_weights that were\n",
    "        # calculated on the last iteration of the loop.\n",
    "        # So, recalculate them outside the loop.\n",
    "        self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "        attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "        return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "941639f721f9e3ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:51:31.441049Z",
     "start_time": "2024-11-15T09:51:31.439354Z"
    }
   },
   "outputs": [],
   "source": [
    "translator = Corrector(tokenizer, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c7562caa0e9b5ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:51:31.856944Z",
     "start_time": "2024-11-15T09:51:31.854987Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_correction(sentence, tokens, ground_truth):\n",
    "    print(f'{\"Input:\":15s}: {sentence}')\n",
    "    print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "    print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b8a6bb0406072a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:51:32.263Z",
     "start_time": "2024-11-15T09:51:32.258906Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self, normalized_transcription, char_set = None, list_of_words = None):\n",
    "        self.normalized_transcription = normalized_transcription\n",
    "        #         self.new_sentences = set()\n",
    "        self.new_sentences = list()\n",
    "    def performe_preprocessing(self):\n",
    "        pattern = r'.*\\d.*'\n",
    "        new_data = [sentence for sentence in self.normalized_transcription if not re.match(pattern, sentence)]\n",
    "        self.normalized_transcription = new_data\n",
    "        if \" \" in self.normalized_transcription:\n",
    "            self.normalized_transcription.remove(\" \")\n",
    "        for sent in self.normalized_transcription:\n",
    "            sent = sent.lower()\n",
    "            new_sent = \"\"\n",
    "            for i in sent:\n",
    "                if ord(i) in range(ord(\"ա\"), ord(\"և\")+1) or i in {\",\", \"։\", \" \", \":\", \",\"}:\n",
    "                    new_sent += i\n",
    "\n",
    "            new_sent = new_sent.lower()\n",
    "\n",
    "            if len(new_sent.strip())>1:\n",
    "                if \":\" in new_sent:\n",
    "                    new_sent = new_sent.replace(\":\", \"։\")\n",
    "                if new_sent[-1] == \"։\":\n",
    "                    new_sent = new_sent.replace(\"։\", \"\")\n",
    "                new_sent = new_sent.replace(\"եվ\", \"և\")\n",
    "                new_sent = new_sent.replace(\"։\", \" ։\")\n",
    "                new_sent = new_sent.replace(\",\", \",\")\n",
    "                \n",
    "                new_sent = new_sent.replace(\",\", \" ,\")\n",
    "                new_sent = new_sent.replace(\",\", \", \")\n",
    "                new_sent = \" \".join(new_sent.split())\n",
    "\n",
    "                self.new_sentences.append(new_sent)\n",
    "        return self.new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "93cfe1621275bc85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:52:11.431529Z",
     "start_time": "2024-11-15T09:52:09.193907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "բառերի քանակը 9\n",
      "տոկենների քանակը 17\n",
      "Inference time: 1.0345 seconds\n",
      "Input:         : երկու հազար քսան թվանը կարղ է լինլ հեղափոխակա տաի\n",
      "Prediction     : երկու հազար քսան թվականը կարող է լինել հեղափոխական դասի\n",
      "Ground truth   : \n"
     ]
    }
   ],
   "source": [
    "sentence = \"երկու հազար քսան թվանը կարղ է լինլ հեղափոխակա տաի\"\n",
    "\n",
    "ground_truth = \"\"\n",
    "\n",
    "print(\"բառերի քանակը\", len(sentence.split()))\n",
    "print(\"տոկենների քանակը\", tokenizer.tokenize(tf.constant([sentence]))[0].shape[0])\n",
    "\n",
    "prp = Preprocessing(normalized_transcription=[sentence])\n",
    "filtered_sentence = prp.performe_preprocessing()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(filtered_sentence))\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "\n",
    "print_correction(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9703230a940b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import random\n",
    "# import pickle\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70ebd2ebcf6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def duplicate_words(sentence):\n",
    "#     words = sentence.split()\n",
    "#     num_words = len(words)\n",
    "#     if num_words <= 1:\n",
    "#         return sentence\n",
    "# \n",
    "#     \n",
    "#     min_duplicates = int(0.05*num_words)\n",
    "#     max_duplicates = int(0.1*num_words)\n",
    "#     num_duplicates = random.randint(min_duplicates, max_duplicates)\n",
    "#     print(num_duplicates)\n",
    "# \n",
    "#     duplicate_indices = random.sample(range(num_words), num_duplicates)\n",
    "#     print(duplicate_indices)\n",
    "#     for idx in sorted(duplicate_indices, reverse=True):\n",
    "#         words.insert(idx, words[idx])\n",
    "# \n",
    "#     return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85748f6930c315bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"Ես գնում եմ տուն և ցանկանում եմ գնալ նաև բնակաչրան\"\n",
    "# len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab767580b424e32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
